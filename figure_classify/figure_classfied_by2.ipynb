{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nhTRLTR//2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a\nHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIy\nMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIA\nAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQA\nAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3\nODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWm\np6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEA\nAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSEx\nBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElK\nU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3\nuLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDE1YS6\nLYWKreSyzRuygKwwQAOWFZMGu3pMwMryCQchiSBzXTw6PqfiDS21U6esVlGS5lnAhUg9SCxG788V\nzkSi7jnTDKFOCyAAEDpznFSxWuA1FY3DCBG3RhuTwvOOKnXWnkjKxRKrc5wT0rOthpwYGd5XYcbc\ncY+orqLGO3SESQRRoCOMLg00wsyhFcak7eaIDtyBkqQKu6nquoafYKFvGEksgDBCcAYqy8zdM1n6\njaG9hMYYK2dwfGcVV7CscnqV7cXsvnysXYjgjms+1fbdZ8zaAOcjNdIbBJwsYkTMKDdlCM56dM1m\nXlh9n3Mm1sjOBnrnHpUhY6O3uZLjw/GL+9v7iR2AaxLkIyg5HU4HQdqxr2W6mmEcyrbxnkRjhQP6\n1TiunPOZjx/X/CoZJWe6/eZ3KwB3VOrOhRppaPUv7YYMyAnA7n+ldLbSBLWMA/wiuUjb7XIVRsIO\nMY61voojjIDEhcKOevrTWhE32LxlJPBoZiVzVUN8gOeSSKmjPmJjvTZmRXtoLmD92EWQcqe2fesG\n50vUmyXljcdcZOK6W1PmMY2+XI+U+9Svbo/DNg9+OB71NxnBxX0iovMeM9Nn/wBekaZppJZnTLPz\nhRxmqasFQYiYyBuuePyrUi8+6maedQCxzxTK0SLWnRbQBtGfpWsSAowAOKqQrsXAqw7bfwFBIvme\nwBqWNyvWqobJp4bBzTEXPMw3HFE1yUTfnI9M1WL96ZK4KEHkH06ipBGNaWyvKuR8uRW1PHGkQG0c\ncLismGREAy4B9zirj3MR2qJlIUf3u9OwE8Y49qbvzkmoluoXJCSxkjggMOtN+0RZYeYm4DJG7pQB\nKeAG9eKOfvbj9KrPdwooLTRgHplhSG+tdn/HzF/32KYi6H45omVkYqfqKpJqFrxm5i/77FS3WqWk\nsYIni3Lx/rBSA//Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a\nHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIy\nMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIA\nAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQA\nAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3\nODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWm\np6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEA\nAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSEx\nBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElK\nU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3\nuLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDzUXk0\nYRkQqAVfGSOc+30ro31PUYtOF+sMxhddwbPA/DripdJ8M6pI0MV7HbCHd+9YuSzgkenOf0r0+ysb\nC2tY7eCPESDADen49aqE5R+EylCMtzwjU759UlikfcXX5QB796dB5n2v7hwYuDtIDHZ69z9K9tu/\nCemXUU7Wxa1nljKEqcr+I9PpXEv8N9d3Ni/s0Q5x+9bP1+7RzSbvLcqMYpaHndp9p/tRNwmMYfOC\nDg//AFqn1T7Q2oXbSbyquQpZcDGO39K3rnRH0DVJLW51O2aaKIHZltr56KOPx9KzNU3vrc1p/aMc\ngMmPMRiY8tjOPbn9KlvTUZDElwZoQsjIkbfNkkYyPT8qeySQLceU4LeXwy/w/MOnNbZ0iwhikN5r\nkB4DKYSCTz7+wrFvLjS4rAPaz3MkkgIdWAXbgjvTU+VK5teCp2tqew2Fqm0ZVfoQK3raGFMZVST7\nVVtIAOdxx9OcdvxNbFvbBpET5iSwXPbP8X4AUrmNizbQxOPu49sUXsbxWshtYvMm2nYpx1xx1Iq7\nbQJ5eQxwVZvwBwKnNqm4/MRgrk+xFA0fLuo6X5GqXUd1IYr1WziZi3PfJ9ai0vQv3m6aJ5pCcLHE\nrY/EnH9a+ktT0u1kCvNbwynJV98YOCPwrLltYYI1EUUSbRgbUAwPaspRb6jseMyeCp5mSWaBbQA4\n2nLbh7gVznimwi0uaK3iuN5K5KhAMV7nqEqxKMuB3IIzkf0rxPxtHFNrsl3C27zMZxz0HrVKCSEz\n3fTzMx3M5ATLZAycmtW3kIAxJIAqlR8o6HrUEUPlQKnQnlhirsEA2jjmmnfUC/auXAHmPggJyB0q\n2cndlnOcA8DnFR2sACjJxzV4xDceO9MZxfjTxBHoGnSXDkyXM3y28RH33PH6V5IPHXiKBg81yZv3\noQo8IAHr0rpPHerW+s+KJLSSUC2tf3Sqw43Z+Y4P+eK5PULawhvUgtnE9vEgeYoxUMx/hGM4wPWu\nj2aULvdmfM3Iv+Jdaububyjvi3chHUoY/c4PNefajODK6Rndz98nOTXWNqUCaiZmhka38sxiNpNx\nAxjrXI6ikfnu0QIUsSoJzx2rOSGmfUoBeTI5571oxRqNuB2rya2+K0zMgGkQs7Zwq3BOPxxRN8Z9\nUts/8UvE6g4+S6JJ/ALWaXYr1Pa4Y/kAq2RyfevFYfjlcrAjz+HoonfOEa5OcfitV0/aEufPMcvh\nqNVHBZLlm/8AZaLMZ6Rr/gTw7r0rXF7p6/aG+9NExRz9cdfxrA1DwXplj4eurLS7FTLJg75Dvckd\nOSfc+3Ncs/x+DggaEQSMf61v8Kzm+N87RMJtGQSkcAMwAOPf8Kd2KxxuoW01rPLbzKVdGIIPb2rC\nuxycVs6j4on125a8vLRlmIxuRMh8fQD2rClu2d5FigZgeFbBzj6VpJ3RKTP/2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 2\n",
    "from IPython.display import Image,display\n",
    "listOfImageName=['./image_64/label_64_64_0/361123201502011309E03655_1.JPG',\n",
    "                './image_64/label_64_64_0/361124201508011107L06095_3.JPG',\n",
    "                './image_64/label_64_64_1/361125201505011209L55757_3.JPG',\n",
    "                './image_64/label_64_64_1/361129201504011309EF7139_1.JPG']\n",
    "for imageName in listOfImageName:\n",
    "    display(Image(filename=imageName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/meicg/Documents/MATLAB/label_64_0\n",
      "/Users/meicg/Documents/MATLAB/label_64_1\n",
      "Full dataset tensor: (19774, 64, 64, 3)\n",
      "Mean: -0.173262\n",
      "Standard deviation: 0.223332\n",
      "Labels: (19774,)\n"
     ]
    }
   ],
   "source": [
    "image_size = 64  # Pixel width and height.\n",
    "channel = 3\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "folders = ['/Users/meicg/Documents/MATLAB/label_64_0',\n",
    "            '/Users/meicg/Documents/MATLAB/label_64_1']\n",
    "\n",
    "def load(data_folders, min_num_images, max_num_images):\n",
    "  dataset = np.ndarray(\n",
    "    shape=(max_num_images, image_size, image_size, channel), dtype=np.float32)\n",
    "  labels = np.ndarray(shape=(max_num_images), dtype=np.int32)\n",
    "  label_index = 0\n",
    "  image_index = 0\n",
    "  for folder in data_folders:\n",
    "    print folder\n",
    "    for image in os.listdir(folder):\n",
    "      if image_index >= max_num_images:\n",
    "        raise Exception('More images than expected: %d >= %d' % (\n",
    "          num_images, max_num_images))\n",
    "      image_file = os.path.join(folder, image)\n",
    "      try:\n",
    "        image_data = (ndimage.imread(image_file).astype(float) -\n",
    "                      pixel_depth / 2) / pixel_depth\n",
    "        if image_data.shape != (image_size, image_size,channel):\n",
    "          raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "        dataset[image_index, :, :, :] = image_data\n",
    "        labels[image_index] = label_index\n",
    "        image_index += 1\n",
    "      except IOError as e:\n",
    "        print 'Could not read:', image_file, ':', e, '- it\\'s ok, skipping.'\n",
    "    label_index += 1\n",
    "  num_images = image_index\n",
    "  dataset = dataset[0:num_images, :, :, :]\n",
    "  labels = labels[0:num_images]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' % (\n",
    "        num_images, min_num_images))\n",
    "  print 'Full dataset tensor:', dataset.shape\n",
    "  print 'Mean:', np.mean(dataset)\n",
    "  print 'Standard deviation:', np.std(dataset)\n",
    "  print 'Labels:', labels.shape\n",
    "  return dataset, labels\n",
    "dataset, labels = load(folders, 10000, 20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/meicg/Desktop/already_ok/four2one_64\n",
      "Full dataset tensor: (1910, 64, 64, 3)\n",
      "Mean: -0.163134\n",
      "Standard deviation: 0.239894\n"
     ]
    }
   ],
   "source": [
    "testFolders = ['/Users/meicg/Desktop/already_ok/four2one_64'];\n",
    "\n",
    "def loadTestImages(data_folders,min_num_images, max_num_images):\n",
    "  dataset = np.ndarray(\n",
    "    shape=(max_num_images, image_size, image_size, channel), dtype=np.float32)\n",
    "  image_index = 0\n",
    "  for folder in data_folders:\n",
    "    print folder\n",
    "    for image in os.listdir(folder):\n",
    "      image_file = os.path.join(folder, image)\n",
    "      try:\n",
    "        image_data = (ndimage.imread(image_file).astype(float) -\n",
    "                      pixel_depth / 2) / pixel_depth\n",
    "        if image_data.shape != (image_size, image_size,channel):\n",
    "          raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "        dataset[image_index, :, :, :] = image_data\n",
    "        image_index += 1\n",
    "      except IOError as e:\n",
    "        print 'Could not read:', image_file, ':', e, '- it\\'s ok, skipping.'\n",
    "  num_images = image_index\n",
    "  dataset = dataset[0:num_images, :, :, :]\n",
    "  print 'Full dataset tensor:', dataset.shape\n",
    "  print 'Mean:', np.mean(dataset)\n",
    "  print 'Standard deviation:', np.std(dataset)\n",
    "  return dataset\n",
    "\n",
    "testDataSet = loadTestImages(testFolders, 1000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "dataset, labels = randomize(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (15000, 64, 64, 3) (15000,)\n",
      "Validation (2350, 64, 64, 3) (2350,)\n",
      "Test (2400, 64, 64, 3) (2400,)\n"
     ]
    }
   ],
   "source": [
    "train_size = 15000\n",
    "valid_size = 2350\n",
    "test_size = 2400\n",
    "\n",
    "valid_dataset = dataset[:valid_size,:,:,:]\n",
    "valid_labels = labels[:valid_size]\n",
    "train_dataset = dataset[valid_size:valid_size+train_size,:,:,:]\n",
    "train_labels = labels[valid_size:valid_size+train_size]\n",
    "test_dataset = dataset[valid_size+train_size:train_size+valid_size+test_size,:,:,:]\n",
    "test_labels = labels[valid_size+train_size:train_size+valid_size+test_size]\n",
    "\n",
    "print 'Training', train_dataset.shape, train_labels.shape\n",
    "print 'Validation', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (15000, 64, 64, 3) (15000, 2)\n",
      "Validation set (2350, 64, 64, 3) (2350, 2)\n",
      "Test set (2400, 64, 64, 3) (2400, 2)\n"
     ]
    }
   ],
   "source": [
    "image_size = 64\n",
    "num_labels = 2\n",
    "num_channels = 3 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 512\n",
    "lamada = 0.02\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  #tf_classify_dataset = tf.constant(testDataSet)\n",
    "  \n",
    "  # Variables.\n",
    "  conv1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  conv1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth*2], stddev=0.1))\n",
    "  conv2_biases = tf.Variable(tf.constant(0.0, shape=[depth*2]))\n",
    "  conv3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth*2, depth*4], stddev=0.1))\n",
    "  conv3_biases = tf.Variable(tf.constant(0.0, shape=[depth*4]))\n",
    "  conv4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth*4, depth*8], stddev=0.1))\n",
    "  conv4_biases = tf.Variable(tf.constant(1.0, shape=[depth*8]))\n",
    "\n",
    "\n",
    "  full_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size / 16 * image_size / 16 * depth*8, num_hidden], stddev=0.1))\n",
    "  full_biases = tf.Variable(tf.constant(0.0, shape=[num_hidden]))\n",
    "  softmax_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  softmax_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "\n",
    "  # Dropout.\n",
    "  keep_prob = tf.placeholder(\"float\")\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv_1 = tf.nn.conv2d(data, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    relu_1 = tf.nn.relu(conv_1 + conv1_biases)\n",
    "    max_pool_1 = tf.nn.max_pool(relu_1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],padding = 'SAME')\n",
    "    \n",
    "    conv_2 = tf.nn.conv2d(max_pool_1, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    relu_2 = tf.nn.relu(conv_2 + conv2_biases)\n",
    "    max_pool_2 = tf.nn.max_pool(relu_2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],padding = 'SAME')\n",
    "    \n",
    "    conv_3 = tf.nn.conv2d(max_pool_2, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    relu_3 = tf.nn.relu(conv_3 + conv3_biases)\n",
    "    max_pool_3 = tf.nn.max_pool(relu_3, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],padding = 'SAME')\n",
    "    \n",
    "    conv_4 = tf.nn.conv2d(max_pool_3, conv4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    relu_4 = tf.nn.relu(conv_4 + conv4_biases)\n",
    "    max_pool_4 = tf.nn.max_pool(relu_4, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],padding = 'SAME')\n",
    "    \n",
    "    shape = max_pool_4.get_shape().as_list()\n",
    "    reshape = tf.reshape(max_pool_4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    full_layer = tf.nn.relu(tf.matmul(reshape, full_weights) + full_biases)\n",
    "    softmax_layer = tf.nn.dropout(full_layer, keep_prob)\n",
    "    return tf.matmul(softmax_layer, softmax_weights) + softmax_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "     \n",
    "  loss = tf.reduce_mean(\n",
    "     tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "     lamada*(tf.nn.l2_loss(conv1_weights) + tf.nn.l2_loss(conv2_weights) + \n",
    "          tf.nn.l2_loss(conv3_weights) + tf.nn.l2_loss(conv4_weights) + \n",
    "          tf.nn.l2_loss(full_weights) + tf.nn.l2_loss(softmax_weights) ))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.02).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "  #classify_prediction = tf.nn.softmax(model(tf_classify_dataset))\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 0.464019\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 100 : 0.521478\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 200 : 0.495593\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 300 : 0.422308\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 400 : 0.42554\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 500 : 0.42019\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 600 : 0.561798\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 700 : 0.424771\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 800 : 0.430948\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 900 : 0.417079\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1000 : 0.428648\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1100 : 0.512445\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1200 : 0.477086\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1300 : 0.50151\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1400 : 0.431941\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1500 : 0.452855\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1600 : 0.411516\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1700 : 0.447433\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1800 : 0.441816\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1900 : 0.411238\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2000 : 0.452606\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2100 : 0.432195\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2200 : 0.449185\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 2300 : 0.377964\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2400 : 0.443329\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500 : 0.442656\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2600 : 0.390096\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2700 : 0.420008\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2800 : 0.551272\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2900 : 0.399724\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3000 : 0.400343\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3100 : 0.382886\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3200 : 0.380956\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3300 : 0.394146\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3400 : 0.409083\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 3500 : 0.395702\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 3600 : 0.418349\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3700 : 0.418952\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3800 : 0.388371\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3900 : 0.449639\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 4000 : 0.399934\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4100 : 0.397206\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 4200 : 0.408384\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4300 : 0.400332\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4400 : 0.406989\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 4500 : 0.390598\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 4600 : 0.367969\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4700 : 0.40323\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 4800 : 0.359178\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4900 : 0.46699\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 5000 : 0.556723\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 75.4%\n",
      "Test accuracy: 77.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  saver.restore(session,'conv_layer4_param8-1000')\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(feed_dict={keep_prob : 1}), valid_labels)\n",
    "    if (step % 1000 == 0):\n",
    "        saver.save(session,'conv_layer4_param9',global_step = step)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(feed_dict={keep_prob : 1}), test_labels)\n",
    "  #classify_result = classify_prediction.eval(feed_dict={keep_prob : 1})\n",
    "\n",
    "# 第一次 训练5000次 Test accuracy 为 87.6%\n",
    "# 第二次 加训10000次 Test accuracy 为 89.6%\n",
    "# 第三次 由于之前有过拟合现象,采用参数regularization. 训练了 5000次 84.2%\n",
    "# 第四次 继续加训5000次 达到 85.6% Minibatch 降到 0.592821 说明没有过拟合\n",
    "# 下面需要对模型进行改进,增加层数.才能提高准确率.\n",
    "# 用四层的conv 获得 accuracy 89.8% Minibatch 的接受度 接近100% 可能还是过拟合,或者测试数据集 有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923952\n"
     ]
    }
   ],
   "source": [
    "print classify_result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/meicg/Desktop/already_ok/four2one_64/']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "testFolders = ['/Users/meicg/Desktop/already_ok/four2one_64/']\n",
    "originalFolder = ['/Users/meicg/Desktop/already_ok/four2one/']\n",
    "classifyFolders = ['/Users/meicg/Desktop/already_ok/label_0/',\n",
    "                   '/Users/meicg/Desktop/already_ok/label_1/']\n",
    "def classifyTestImages(small_data_folders, original_data_folder, classify_folders ):\n",
    "    print small_data_folders \n",
    "    flag = 0\n",
    "    for image in os.listdir(small_data_folders[0]):\n",
    "        image_file_origin  = os.path.join(original_data_folder[0], image)\n",
    "        image_file_destination_0 = os.path.join(classify_folders[0], image )\n",
    "        image_file_destination_1 = os.path.join(classify_folders[1], image )\n",
    "        if classify_result[flag][0] >=0.5 :\n",
    "            shutil.copyfile(image_file_origin ,image_file_destination_0)\n",
    "        elif classify_result[flag][1] >0.5 :\n",
    "            shutil.copyfile(image_file_origin ,image_file_destination_1)\n",
    "        flag += 1\n",
    "    return flag\n",
    "\n",
    "flag2 = classifyTestImages(testFolders, originalFolder,classifyFolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762\n"
     ]
    }
   ],
   "source": [
    "print flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
